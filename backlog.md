# Backlog

## Improve LLM Creativity

### ChatGPT Suggestions

- Use model chaining, where each model contributes a piece of the reasoning pipeline:

  | Layer                 | Role                                                                              | Recommended Model                         |
  | --------------------- | --------------------------------------------------------------------------------- | ----------------------------------------- |
  | **Context Builder**   | Extracts or interprets song context (e.g. `"this section is intense and rising"`) | `Mistral`, `Phi-3`, `Mixtral`             |
  | **Lighting Planner**  | Proposes creative, mood-aligned lighting effects                                  | `GPT-4`, `Qwen-1.5-32B-Chat`, `Yi-34B`        |
  | **Effect Translator** | Converts symbolic actions into JSON/DMX format                                    | `Mistral`, `Code LLM`    |

- try these local models

  | Model                  | Strength                                                              |
  | ---------------------- | --------------------------------------------------------------------- |
  | `Yi-34B`               | Open-ended creative language generation                               |
  | `Qwen-1.5-32B-Chat`    | Emotionally rich, good at visual metaphor                             |
  | `Command-R`            | Good for following task-specific instructions (e.g., generate 3 cues) |
  | `LLaVA` + spectrograms | Add vision input if you extract song visuals or waveform sections     |


- Train or use a model like MusicLM, CLAP, or jukebox to embed the song section and retrieve matching lighting prompts from a database using similarity search (e.g., FAISS or ChromaDB).


Use Essentia to extract time series ‚Üí feed compact summaries into LLM to label moments.

üß© Pipeline Sketch

1. Preprocess audio with Essentia (or similar):
  - Compute sliding-window features (e.g., every 1s):
  - RMS (loudness)
  - Spectral centroid (brightness)
  - Onset rate (percussiveness)
  - MFCC variance (texture)
  - Silence ratio
  - Vocal energy (from stems)
  - Drum/bass/vocal stem levels

2. Identify candidate windows:
  - Local peaks, valleys, or change deltas in features.
  - Heuristics like:
    ```python
    if energy[t+1] - energy[t-1] > threshold and vocals[t] < 0.2:
        candidate = t
    ```

3. For each candidate window (e.g. 4s around t), prepare a natural language summary:
    ```
    From 96s to 100s:
    - Drums increase in energy
    - Vocals are absent
    - Brightness (spectral centroid) increases
    - Tempo stays constant
    ```
    
4. Prompt LLM:

    ```
    What kind of musical event is happening here?
    ```

    Expected reply:

    ```
    ‚ÄúPossible Drop or Instrumental Break‚Äù
    ```

5. Collect answers into draft key_moments json


-----------------------------------------------------------------------------------------------------------------------------------------------


### ‚¨ú Implement 3-Agent LangGraph Pipeline: Context ‚Üí Lighting Plan ‚Üí DMX Translation

**Goal**: Use LangGraph to chain 3 AI-based lighting planning agents:
1. **Context Builder** (Mixtral): interprets the song segment
2. **Lighting Planner** (Mixtral): proposes symbolic lighting actions
3. **Effect Translator** (Command-R): outputs final DMX-ready data

---

#### üß† Tasks

**Create file**: `backend/services/langgraph/lighting_pipeline.py`  
Implement a LangGraph pipeline with these 3 nodes:

---

### üî∑ 1. Node: `context_builder` (Mixtral)

```python
def run_context_builder(state: Dict) -> Dict:
Input:

```
{
  "segment": {
    "name": "Drop",
    "start": 34.2,
    "end": 36.7,
    "features": {
      "bpm": 128,
      "energy": 0.92,
      "instrumentation": ["kick", "bass", "synth"]
    }
  }
}
```

Prompt:
```python

```
You are a music context interpreter.
Describe the emotional and sonic feel of this section:

Segment: Drop
Start: 34.2s
End: 36.7s
Features: {"bpm": 128, "energy": 0.92, "instrumentation": ["kick", "bass", "synth"]}

Respond with a short natural language summary like:
"High energy climax with heavy bass and bright synth"
Model: Mixtral (use Ollama to call Mixtral model)

Output:

```json
{ "context_summary": "..." }
üî∑ 2. Node: lighting_planner (Mixtral)
```python
def run_lighting_planner(state: Dict) -> Dict:
Input:

```json
{ "context_summary": "...", "segment": {...} }
```

Prompt:

```text
You are a stage lighting designer.

Based on this musical context:
"High energy climax with heavy bass and bright synth"

Suggest lighting actions. Use "flash", "strobe", "fade", etc.
Return JSON array with:
- type
- color
- start (seconds)
- duration (seconds)

This section starts at: 34.2s and ends at 36.7s
```

Model: Mixtral

Output example:

```json
{
  "actions": [
    { "type": "strobe", "color": "white", "start": 34.2, "duration": 2.5 },
    { "type": "flash", "color": "blue", "start": 35.0, "duration": 1.0 }
  ]
}

üî∑ 3. Node: effect_translator (Command-R)
```python
def run_effect_translator(state: Dict) -> Dict:
Input:

```json
{ "actions": [ ... ] }
Prompt:
```text
You are a lighting control expert.

Given a list of symbolic lighting actions like:
[
  { "type": "strobe", "color": "white", "start": 34.2, "duration": 2.5 },
  { "type": "flash", "color": "blue", "start": 35.0, "duration": 1.0 }
]

Convert these into low-level DMX fixture instructions for:
- Fixture: "parcan_l" uses channel 10 for color/strobe
- Fixture: "head_el150" uses presets like "Drop"

Output JSON with a "dmx" array:
[
  { "fixture": "parcan_l", "channel": 10, "value": 255, "time": 34.2 },
  { "fixture": "head_el150", "preset": "Drop", "time": 35.0 }
]
Model: Command-R (use Ollama to call Command-R model)

Output:

```json
{
  "dmx": [
    { "fixture": "parcan_l", "channel": 10, "value": 255, "time": 34.2 },
    { "fixture": "head_el150", "preset": "Drop", "time": 35.0 }
  ]
}
```
üîÅ LangGraph Setup
Use StateGraph() from langgraph.graph

Add all 3 nodes

Set entry: context_builder

Set finish: effect_translator

Chain edges between nodes

Function to call:

```python
result = pipeline.invoke(test_segment_input)
‚úÖ Success Criteria
The pipeline runs end-to-end from raw segment input ‚Üí final DMX plan

Each node output is saved to logs/{node_name}.json

All inputs/outputs are dictionaries with string/float/list values

Use Mixtral for Node 1 & 2, Command-R for Node 3