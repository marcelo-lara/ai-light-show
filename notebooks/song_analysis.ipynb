{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c3e409",
   "metadata": {},
   "source": [
    "# Audio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link backend\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append('/home/darkangel/ai-light-show/backend')\n",
    "from backend.config import SONGS_TEMP_DIR, SONGS_DIR\n",
    "\n",
    "# sample song \n",
    "song_name = 'Queen of Kings - Alessandra'\n",
    "songs_folder = \"/home/darkangel/ai-light-show/songs/\"\n",
    "song_file = f\"{songs_folder}{song_name}.mp3\"\n",
    "print(f\"song: {song_name} | {song_file}\")\n",
    "\n",
    "# load metadata\n",
    "from backend.song_metadata import SongMetadata\n",
    "song = SongMetadata(song_name=song_name, songs_folder=songs_folder)\n",
    "print(f\"song metadata: {song}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.ai.demucs_split import extract_stems\n",
    "stems_folder = extract_stems(song_file)\n",
    "\n",
    "drums_path = f\"{stems_folder['output_folder']}/drums.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"drums: {drums_path}\")\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "y, sr = librosa.load(drums_path, sr=None)\n",
    "y = y / np.max(np.abs(y))  # peak normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib --quiet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 🔍 Plot waveform\n",
    "plt.figure(figsize=(14, 4))\n",
    "librosa.display.waveshow(y, sr=sr, alpha=0.6)\n",
    "plt.title(\"Waveform of drums.wav\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "num_points = len(y)\n",
    "times = np.linspace(0, num_points / sr, num_points)\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Plot spectrogram\n",
    "S = librosa.stft(y, n_fft=2048, hop_length=512)\n",
    "S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "plt.figure(figsize=(14, 6))\n",
    "librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram of drums.wav')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch, librosa, numpy as np\n",
    "\n",
    "model_name = \"yojul/wav2vec2-base-one-shot-hip-hop-drums-clf\"\n",
    "feat = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "y, sr = librosa.load(drums_path, sr=16000, mono=True)\n",
    "window, hop = int(0.1*sr), int(0.05*sr)\n",
    "events = []\n",
    "\n",
    "for start in range(0, len(y)-window, hop):\n",
    "    chunk = y[start:start+window]\n",
    "    inputs = feat(chunk, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    label = model.config.id2label[np.argmax(probs)]\n",
    "    conf = float(np.max(probs))\n",
    "    if conf > 0.6:  # tune down if needed\n",
    "        events.append({\"time\": (start+window/2)/sr, \"type\": label, \"confidence\": conf})\n",
    "\n",
    "print(events[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba323705",
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save events to song metadata\n",
    "drums_events = []\n",
    "unique_types = set(e['type'] for e in events)\n",
    "for t in unique_types:\n",
    "    drums_events.append({\n",
    "        \"type\": t,\n",
    "        \"time\": [e['time'] for e in events if e['type'] == t]\n",
    "        })\n",
    "\n",
    "song.drums = drums_events\n",
    "song.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63706f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --quiet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert events to a DataFrame\n",
    "df = pd.DataFrame(events)\n",
    "pivot_df = df.pivot_table(index=\"type\", columns=\"time\", values=\"confidence\", aggfunc=\"max\", fill_value=0)\n",
    "pivot_df\n",
    "\n",
    "# Plot the heatmap\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_df, cmap=\"YlOrBr\", annot=True, fmt=\".2f\", cbar_kws={'label': 'Confidence'})\n",
    "plt.title(\"Drum Events Heatmap\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Drum Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages and clone the repository\n",
    "!pip install tensorflow keras --quiet\n",
    "!git clone https://github.com/aabalke33/drum-audio-classifier.git\n",
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e84140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🥁 aabalke33 Drum Detection (Keras 3 compatible)\n",
    "!pip install -q tensorflow librosa matplotlib soundfile\n",
    "\n",
    "# Clone model if needed\n",
    "import os\n",
    "if not os.path.exists(\"drum-audio-classifier\"):\n",
    "    !git clone https://github.com/aabalke33/drum-audio-classifier.git\n",
    "\n",
    "import librosa, numpy as np, json, tensorflow as tf\n",
    "from keras.layers import TFSMLayer\n",
    "\n",
    "# Load model using TFSMLayer\n",
    "model_path = \"drum-audio-classifier/saved_model/model_20230607_02\"\n",
    "layer = TFSMLayer(model_path, call_endpoint=\"serving_default\")\n",
    "class_names = ['Clap', 'Closed_Hi-Hat', 'Kick', 'Open_Hi-Hat', 'Snare']\n",
    "\n",
    "# Load audio\n",
    "y, sr = librosa.load(\"drums.wav\", sr=16000, mono=True)\n",
    "window, hop = int(0.1 * sr), int(0.05 * sr)\n",
    "events = []\n",
    "\n",
    "# Detect events using sliding window + spectrogram\n",
    "for start in range(0, len(y) - window, hop):\n",
    "    chunk = y[start:start+window]\n",
    "    mel = librosa.feature.melspectrogram(chunk, sr=sr, n_mels=128, fmax=8000)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel_db = np.stack([mel_db]*3, axis=-1)\n",
    "    mel_db = tf.image.resize(mel_db, (128, 128))\n",
    "    mel_db = tf.expand_dims(mel_db, axis=0)\n",
    "\n",
    "    pred = layer(mel_db)[0].numpy()\n",
    "    label = class_names[np.argmax(pred)]\n",
    "    conf = float(np.max(pred))\n",
    "    if conf > 0.5:\n",
    "        events.append({\"time\": (start + window/2) / sr, \"type\": label, \"confidence\": conf})\n",
    "\n",
    "# Save results\n",
    "with open(\"aabalke_events.json\", \"w\") as f:\n",
    "    json.dump(events, f, indent=2)\n",
    "\n",
    "print(f\"✅ Saved aabalke_events.json with {len(events)} events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd346d2",
   "metadata": {},
   "source": [
    "## Use pretzel-ai/drum-transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers flax jax librosa soundfile --quiet\n",
    "\n",
    "\n",
    "# load model and tokenizer\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch, librosa, numpy as np\n",
    "\n",
    "model_name = \"DunnBC22/wav2vec2-base-Drum_Kit_Sounds\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load drums.wav\n",
    "y, sr = librosa.load(drums_path, sr=16000, mono=True)\n",
    "y = y / np.max(np.abs(y))  # peak normalize\n",
    "\n",
    "window = int(0.1 * sr)\n",
    "hop = int(0.05 * sr)\n",
    "\n",
    "events = []\n",
    "for start in range(0, len(y) - window, hop):\n",
    "    chunk = y[start:start+window]\n",
    "    inputs = feature_extractor(chunk, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    label = model.config.id2label[np.argmax(probs)]\n",
    "    confidence = float(np.max(probs))\n",
    "\n",
    "    if confidence > 0.3:\n",
    "        time_sec = (start + window/2) / sr\n",
    "        events.append({\"time\": time_sec, \"type\": label, \"confidence\": confidence})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ac70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(events)\n",
    "df = df.sort_values(by='time').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2325684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a single random 1-second chunk\n",
    "test_chunk = y[sr * 44:sr * 46]  # 10s to 11s\n",
    "inputs = feature_extractor(test_chunk, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "top_idx = np.argmax(probs)\n",
    "label = model.config.id2label[top_idx]\n",
    "confidence = float(probs[top_idx])\n",
    "\n",
    "print(f\"Predicted label: {label}, confidence: {confidence:.3f}\")\n",
    "\n",
    "for idx, prob in enumerate(probs):\n",
    "    label = model.config.id2label[idx]\n",
    "    print(f\"{label}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12437230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
