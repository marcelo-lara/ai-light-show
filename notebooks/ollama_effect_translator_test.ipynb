{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Translator Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Song Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.models.song_metadata import SongMetadata\n",
    "song = SongMetadata('born_slippy', songs_folder='/home/darkangel/ai-light-show/songs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from backend.models.fixtures import FixturesListModel\n",
    "from backend.services.dmx.dmx_canvas import DmxCanvas\n",
    "from backend.config import FIXTURES_FILE\n",
    "dmx_canvas = DmxCanvas()\n",
    "fixtures = FixturesListModel(fixtures_config_file=FIXTURES_FILE, dmx_canvas=dmx_canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Effect Translator Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures_details = []\n",
    "for fixture in fixtures:\n",
    "    fd = fixture.to_dict()\n",
    "    fd[\"action_list\"] = []\n",
    "    for action in fixture.actions:\n",
    "        if action==\"arm\":\n",
    "            continue\n",
    "        fd[\"action_list\"].append(action)\n",
    "    fixtures_details.append(fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_list = {}\n",
    "for fixture in fixtures:\n",
    "    for action_name, data in fixture.action_models.items():\n",
    "        actions_list[action_name] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats = song.get_beats_array() \n",
    "beats = [beat for beat in beats if beat < 35]  # Filter out beats that are too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "templates_folder = Path('/home/darkangel/ai-light-show/backend/services/agents/prompts')\n",
    "prompt_template = 'effect_translator.j2'\n",
    "\n",
    "# Load the Jinja2 template\n",
    "import jinja2\n",
    "template_loader = jinja2.FileSystemLoader(searchpath=templates_folder)\n",
    "template_env = jinja2.Environment(loader=template_loader)\n",
    "template = template_env.get_template(prompt_template)\n",
    "\n",
    "# render\n",
    "instructions = template.render(song=song, fixtures=fixtures_details, actions_list=actions_list, beats=beats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Call Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Ollama API parameters\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"gemma3n:e4b\"\n",
    "\n",
    "prompt = \"a flash from left to right on every beat from start to 12 seconds\"\n",
    "full_prompt = f\"{instructions}\\n\\nUser Request: {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the Ollama server and show response\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def call_ollama(instructions, prompt, model=OLLAMA_MODEL, url=OLLAMA_URL):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Ollama server and return the response\n",
    "    \n",
    "    Args:\n",
    "        instructions (str): The system instructions/context for the model\n",
    "        prompt (str): The user prompt to send to the model\n",
    "        model (str): The Ollama model to use\n",
    "        url (str): The URL of the Ollama server\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response text\n",
    "    \"\"\"\n",
    "    api_endpoint = f\"{url}/api/generate\"\n",
    "    \n",
    "    # Combine instructions and prompt\n",
    "    full_prompt = f\"{instructions}\\n\\nUser Request: {prompt}\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": full_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_endpoint, json=payload)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        result = response.json()\n",
    "        return result.get(\"response\", \"No response found\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error calling Ollama API: {str(e)}\"\n",
    "\n",
    "# Use our rendered template as the prompt\n",
    "print(f\"Calling {OLLAMA_MODEL} with the rendered template...\")\n",
    "response = call_ollama(instructions, prompt)\n",
    "print(f\"Response received. Length: {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
